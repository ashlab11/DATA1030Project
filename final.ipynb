{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'm going to download **ALL** the packages I need for the rest of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from scipy.sparse import csr_array\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malpractice = pd.read_csv(\"medicalMalpractice.csv\")\n",
    "\n",
    "print(malpractice['Amount'].describe())\n",
    "print(malpractice['Age'].value_counts())\n",
    "print(malpractice['Gender'].value_counts())\n",
    "print(malpractice['Insurance'].value_counts())\n",
    "print(malpractice['Marital Status'].value_counts())\n",
    "print(malpractice['Private Attorney'].value_counts())\n",
    "print(malpractice['Severity'].value_counts())\n",
    "print(malpractice['Specialty'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data = malpractice, x = \"Insurance\",y = \"Amount\", cut =0, palette='viridis')\n",
    "plt.xticks(rotation = 35)\n",
    "plt.yticks([0.,  200000.,  400000.,  600000.,  800000.,\n",
    "        1000000.], [0, 2, 4, 6, 8, 10])\n",
    "plt.title(\"Claim amount vs Insurance type\")\n",
    "plt.xlabel(\"Insurance Type\")\n",
    "plt.ylabel(\"Claim amount (in $100000)\")\n",
    "plt.show()\n",
    "\n",
    "sns.violinplot(data = malpractice, x = \"Severity\", y = \"Amount\", cut = 0, palette='viridis')\n",
    "plt.yticks([0.,  200000.,  400000.,  600000.,  800000.,\n",
    "       1000000.], [0, 2, 4, 6, 8, 10])\n",
    "plt.title(\"Severity of claim vs Amount of claim\")\n",
    "plt.ylabel(\"Amount (in $100000)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist((malpractice['Amount']), bins = 500, log=True)\n",
    "plt.title(\"Log number of claims vs Price of Claim\")\n",
    "plt.xlabel(\"Price of claim ($)\")\n",
    "plt.ylabel(\"Log number of claims\")\n",
    "plt.axvline(x = 225000, color = 'darkred', label = 'axvline - full height')\n",
    "plt.savefig(fname = 'NumClaims.jpeg', dpi = 300)\n",
    "plt.show()\n",
    "\n",
    "#Creating a bar plot showing the percent of each severity that has an amount greater than 250k\n",
    "\n",
    "malpractice['Below225000'] = (malpractice['Amount'] <= 225000)\n",
    "severity_graph = pd.DataFrame(malpractice.groupby('Severity')['Below225000'].agg('mean'))\n",
    "severity_graph.reset_index(inplace=True)\n",
    "severity_graph['Above225000'] = 1 - severity_graph['Below225000']\n",
    "severity_graph.set_index('Severity',inplace=True)\n",
    "severity_graph.plot(kind = \"bar\", stacked=True, color = ['darkgreen', \"goldenrod\"])\n",
    "plt.title(\"Distribution of claims by Severity Rating\")\n",
    "plt.xlabel(\"Severity Rating (1 = emotional trauma, 9 = death)\")\n",
    "plt.ylabel(\"Percent (%) of claims\")\n",
    "plt.xticks(rotation = 0)\n",
    "plt.yticks([0. , 0.2, 0.4, 0.6, 0.8, 1.], [0, 20, 40, 60, 80, 100])\n",
    "plt.legend(bbox_to_anchor =(0.5,-0.27), loc='lower center', labels = ['<$225k', '>$225k'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes from EDA\n",
    "\n",
    "Insurance and Marital Status both have missing values \n",
    "\n",
    "Amount is bimodal, with most between 0 and 200k but then some others having suits from 200k to 900k\n",
    "\n",
    "Two different theoretical projects:\n",
    "1. Predicting suit price by regression\n",
    "2. Predicting whether a suit will be for more than 200k by classification\n",
    "\n",
    "Now, I'll deal with some of the errors I found during EDA:\n",
    "- Changing marital status and private attorney from numbers to words describing the status\n",
    "- Changing \"unknown\" to np.NaN so I can work with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malpractice.loc[malpractice['Insurance'] == \"Unknown\", \"Insurance\"] = np.NaN\n",
    "malpractice.loc[malpractice['Marital Status'] == 0, \"Marital Status\"] = \"Divorced\"\n",
    "malpractice.loc[malpractice['Marital Status'] == 1, \"Marital Status\"] = \"Single\"\n",
    "malpractice.loc[malpractice['Marital Status'] == 2, \"Marital Status\"] = \"Married\"\n",
    "malpractice.loc[malpractice['Marital Status'] == 3, \"Marital Status\"] = \"Widowed\"\n",
    "malpractice.loc[malpractice['Marital Status'] == 4, \"Marital Status\"] = np.NaN\n",
    "malpractice.loc[malpractice['Private Attorney'] == 0, \"Private Attorney\"] = \"Not Private\"\n",
    "malpractice.loc[malpractice['Private Attorney'] == 1, \"Private Attorney\"] = \"Private\"\n",
    "\n",
    "print(np.mean(pd.isna(malpractice['Marital Status'])))\n",
    "print(np.mean(pd.isna(malpractice['Insurance'])))\n",
    "print(malpractice.isnull().any(axis = 'columns').mean())\n",
    "\n",
    "X = malpractice.drop(columns=['Amount', 'Below225000'])\n",
    "y = malpractice['Amount'].copy()\n",
    "y[y <= 250000] = 0\n",
    "y[y > 250000] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the transformer to place into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_fts = ['Private Attorney', 'Marital Status', 'Specialty', 'Insurance', 'Gender']\n",
    "\n",
    "std_fts = ['Age']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(), one_hot_fts), \n",
    "        ('std', StandardScaler(), std_fts)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is the meat of the operation. It **takes in X, y, a model, and a dictionary of hyperparameters**. It then returns a list of the best test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StratModelCreator(X, y, model, param_dict: dict, n_states = 4, tree = False):\n",
    "    #the following code makes it so I don't have to append model__ before every hyperparameter\n",
    "    model_name = model.__class__.__name__\n",
    "    param_dict = {f\"{model_name.lower()}__{key}\": value for key, value in param_dict.items()}\n",
    "\n",
    "    test_scores = []\n",
    "    for state in np.linspace(0, n_states - 1, n_states, dtype = int):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=state)    \n",
    "        cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=state)\n",
    "        pipeline = make_pipeline(preprocessor, model)\n",
    "            \n",
    "        #grid = GridSearchCV(pipeline, param_grid = param_dict, scoring = 'f1', cv = cv, verbose = 1, n_jobs=-3, error_score='raise')\n",
    "\n",
    "        grid = RandomizedSearchCV(pipeline, param_dict, n_iter = 150,\n",
    "                        scoring = 'f1', cv = cv, verbose = 1, n_jobs=-3)\n",
    "        grid.fit(X_train, y_train)\n",
    "        y_preds = grid.predict(X_test)\n",
    "        test_scores.append(accuracy_score(y_test, y_preds))    \n",
    "        \n",
    "        #In the final model run-through, use Shap Explainers to get global feature importance\n",
    "        if state == 0:\n",
    "            preprocessor.fit(X_train)\n",
    "            transformed_test = shap.sample(preprocessor.transform(X_test).toarray(), 500, random_state=state)\n",
    "\n",
    "            #tree models can use the TreeExplainer SHAP, but other models just use the regular SHAP\n",
    "            if tree:\n",
    "                explainer = shap.Explainer(grid.best_estimator_[-1])\n",
    "            else:\n",
    "                explainer = shap.Explainer(lambda x: grid.best_estimator_[-1].predict_proba(x)[:, 1], transformed_test)\n",
    "            \n",
    "            shap_values = explainer.shap_values(transformed_test)\n",
    "            shap.summary_plot(shap_values, pd.DataFrame(transformed_test, columns=preprocessor.get_feature_names_out()), plot_type=\"bar\", \n",
    "                            max_display = 6, show = False)\n",
    "            plt.title(f\"Most important features in the {model_name} model\")\n",
    "            plt.xlabel(\"Average impact on model prediction\")\n",
    "            plt.savefig(f\"{model_name}shap.png\", dpi = 300)\n",
    "            plt.show()    \n",
    "            \n",
    "            conf = confusion_matrix(y_test, y_preds, normalize='true')\n",
    "            display = ConfusionMatrixDisplay(confusion_matrix=conf, display_labels = ['Small suit', 'Large suit'])\n",
    "            display.plot()\n",
    "            plt.title(f\"Confusion Matrix for {model_name}\")\n",
    "            plt.savefig(f\"{model_name}confmatrix.png\", dpi = 300)\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "    return test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--LogisticRegression\n",
    "logreg_param_dict = {\n",
    "    'penalty': ['elasticnet'], \n",
    "    'C': np.logspace(-3, 3, 2), \n",
    "    'l1_ratio': np.linspace(0.1, 0.9, 2)\n",
    "}\n",
    "\n",
    "logreg = LogisticRegression(max_iter = 750, solver = 'saga')\n",
    "logreg_scores = StratModelCreator(X, y, logreg, param_dict=logreg_param_dict, tree=False)\n",
    "\n",
    "\n",
    "#--- K-nearest neighbors\n",
    "knn_dict = {\n",
    "    'n_neighbors': np.linspace(10, 50, 10, dtype=int),\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_scores = StratModelCreator(X, y, knn, param_dict=knn_dict, tree = False)\n",
    "\n",
    "\n",
    "#-- XGBoost\n",
    "#list of parameters to loop through for XGBoost\n",
    "xgb_param_dict = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': np.linspace(3, 12, 3, dtype=int),\n",
    "    'n_estimators': [25, 50, 75],\n",
    "    'gamma': [0.01, 0.05, 0.1, 0.2], \n",
    "    'reg_alpha': [0.01, 0.1, 1, 10], \n",
    "    'reg_lambda': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "xgb = xgboost.XGBClassifier()\n",
    "xgb_scores = StratModelCreator(X, y, xgb, param_dict=xgb_param_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a graph to analyze age's effect on the size of a claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data = malpractice, x = \"Below225000\", y = \"Age\", cut = 0, palette='viridis')\n",
    "plt.title(\"Age of person vs size of claim\")\n",
    "plt.ylabel(\"Age of claimant\")\n",
    "plt.xticks([True, False], ['Small Claim', 'Large Claim'])\n",
    "plt.xlabel(\"Size of claim\")\n",
    "plt.savefig(\"AgeGraph.png\", dpi = 300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_test = [0.8926271935361697, 0.890922863274839, 0.8926271935361697, 0.8935740436813534]\n",
    "logreg_best_estimator = LogisticRegression(C=63.0957344480193, l1_ratio=0.0,\n",
    "                                    max_iter=1000, penalty='elasticnet',\n",
    "                                    solver='saga')\n",
    "\n",
    "knn_test = [0.921222067920717, 0.9261456886756723, 0.9250094685014518, 0.925325085216513]\n",
    "knn_best_estimator = KNeighborsClassifier(n_neighbors=27, weights='uniform')\n",
    "\n",
    "xgb_test = [0.927092538820856, 0.9289231157682111, 0.9286074990531499, 0.9291124857972478]\n",
    "xgb_best_estimator = xgboost.XGBClassifier(gamma=0.01, learning_rate=0.1,\n",
    "                               max_depth=12, n_estimators=75)\n",
    "\n",
    "means = [np.mean(logreg_test), np.mean(knn_test), np.mean(xgb_test)]\n",
    "errors = np.multiply((1.965 / np.sqrt(2)), [np.std(logreg_test), np.std(knn_test), np.std(xgb_test)])\n",
    "\n",
    "plt.bar([\"LogReg\", \"KNN\", \"XGBoost\"], means)\n",
    "plt.errorbar([\"LogReg\", \"KNN\", \"XGBoost\"], means, yerr=errors, fmt = \".\", color = \"black\", elinewidth=2,capthick=10,errorevery=1)\n",
    "plt.title(\"Accuracy score for various models\")\n",
    "plt.ylabel(\"Accuracy score\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
